\documentclass{article}

\def\sectionnumber{6}
\def\sectiontitle{Moments; Joint, Conditional, and Marginal Distributions}

\usepackage{common}

% Set this false to hide, true to show
\setboolean{showanswers}{true}

\begin{document} 

\header

\section{Moments and Moment Generating Functions}

\begin{description}
\item[Moments] The $n$th moment of a r.v.~$X$ is $E(X^n)$. The first 4 moments are mean, variance, skewness (measures symmetry), and kurtosis (measures fatness of tails).

\item[Moment generating function] The \textit{moment generating function} (MGF) of an r.v.~$X$ is $M(t) = E(e^{tX})$, as a function of $t$, if this is finite on some open interval $(-a, a)$ containing 0. Otherwise we say the MGF of $X$ does not exist. $t$ itself has no meaning; to find the $n$th moment, we find $M^{(n)}(0) = E(X^n)$, the $n$th derivative at 0. The MGF uniquely determines the distribution. 

\item[Taylor series of MGF] The MGF can also be expressed as $$M(t) = \sum_{n = 0}^\infty M^{(n)}(0)\frac{t^n}{n!} = \sum_{n = 0}^\infty E(X^n)\frac{t^n}{n!}$$ If you can express an MGF in terms of a Taylor series, you automatically know all the moments by pattern-matching.

\item[Sum of independent r.v.s] The MGF of $X + Y$ with $X$, $Y$ independent is $M_X(t)M_Y(t)$.
\end{description}

1. Let $X \sim N(\mu,\sigma^2)$. What is the MGF of $X$ given that the MGF of a standard Normal is given by $M(t) = e^{\frac{t^2}{2}}$ for all $t$? 

\hide{Let $M_X(t)$ be the MGF of $X$. Then by definition, $M_X(t) = E(e^{Xt})$. But if $Z \sim N(0,1)$, $X$ has the same distribution as $\mu + \sigma Z$, so we have $$
E(e^{Xt}) = E(e^{(\mu+\sigma Z)t}) = E(e^{\mu t}e^{\sigma Zt}) = e^{\mu t}M(\sigma t) = e^{\mu t + \frac{1}{2}\sigma^2t^2}
$$}

2. (a) What is the MGF of $U \sim \Unif(a,b)$? For what values of $t$ does it exist?  

\hide{Let $M(t)$ be the desired MGF. Then by definition, $M(t) = E(e^{tU}) = \int_a^b e^{tu} \frac{1}{b-a} du$ by LOTUS. This evaluates to $\boxed{\frac{e^{bt}-e^{at}}{t(b-a)}}$ when $t \neq 0$. When $t = 0$, the integral evaluates to simply $1$. Since the expression we obtain is finite for all $t$, the MGF exists everywhere.}

(b) Now suppose $U_1$ and $U_2$ are independently $\Unif(a,b)$. Prove that the distribution of $U_1+U_2$ is not uniform using your answer in part (a).

\hide{The MGF of $U_1+U_2$ is simply $M^2(t)$, which clearly doesn't take the same form as $M(t)$.}

(c) Now suppose $a=0$ and $b=1$. Find the moments $E(U^n)$ for all positive integers $n$. 

\hide{The MGF simplifies to $M(t) = \frac{e^t-1}{t}$ which has series expansion $\sum_{n=0}^{\infty} \frac{t^n}{(n+1)!}$. The $n$-th moment $E(U^n)$ is the coefficient in front of $\frac{t^n}{n!}$, which here is just $\boxed{\frac{1}{n+1}}$. }


\section{Joint Distributions}

\begin{description}

\item[Joint PMF] $p_{X, Y}(x, y) = P(X=x, Y=y)$

\item[Marginal PMF] $p_{X}(x) = \sum_y P(X=x, Y=y)$.

\item[Conditional PMF] $P(X=x|Y=y)$

\item[Continuous analogues] Replace all the PMFs with PDFs, and replace all the sums with integrals.


\end{description}

3. (BH 7.9) Let $X$ and $Y$ be i.i.d $\Geom(p)$, and $N = X+Y$. 
(a) Find the joint PMF of $X$, $Y$, and $N$. 

\hide{We want to compute $P(X=x,Y=y,N=n)$ for all nonnegative integers $x$, $y$, $n$. If $n \neq x+y$, then this probability is clearly zero, so the PMF is only nonzero when $n = x+y$. But then $N=n$ is redundant information, so $P(X=x,Y=y,N=n) = P(X=x,Y=y) = P(X=x)P(Y=y) = p^2(1-p)^{x+y} = \boxed{p^2(1-p)^{n}}$ where we used independence for the second equality.}

(b) Find the joint PMF of $X$ and $N$. 

\hide{$P(X=x,N=n) = P(X=x,Y=n-x) = P(X=x)P(Y=n-x) = \boxed{p^2(1-p)^n}$ for all nonnegative integers $x$ and $n$ with $n \geq x$. Note that this is the same as above...since you only need any two of $X$, $Y$, and $N$ to get all the information in the problem.}

(c) Find the conditional PMF of $X$ given $N=n$, and give a simple description of what the result says. 

\hide{$P(X=x|N=n) = \frac{P(X=x,N=n)}{P(N=n)}$ by the definition of conditional probability. $N \sim \NBin(2, p)$, so by the PMF of the negative binomial we have $P(N=n) = (n+1)p^2(1-p)^n$. Plugging our result from part (b) for the numerator gives $\boxed{\frac{1}{n+1}}$ for all $x \in \{0,1,\dots,n\}$. This means that it is equally likely for $X$ to be any of these values.}

4. Consider choosing a random point in the unit sphere:
$$\{(x, y, z) : x^2 + y^2 + z^2 \leq 1\}.$$ We can represent this ``random choice" as three random variables, each representing a coordinate: $(X, Y, Z)$.

(a) Find the joint PDF of $X, Y, Z$.

\hide{Since the total volume of the sphere is $\frac{4\pi}{3}$, we must have the PDF as $f_{X,Y,Z}(x, y, z) = \frac{3}{4\pi}$ due to the uniform nature of the distribution.}

(b) Find the marginal distribution of $X$.

\hide{To find the marginal distribution, we must integrate out the other random variables $Y, Z$,
resulting in:

$$\int_{-\sqrt{1 - x^2}}^{\sqrt{1 - x^2}} \int_{-\sqrt{1 - x^2 - z^2}}^{\sqrt{1 - x^2 - z^2}} \frac{3}{4\pi} dydz$$
which is best left as an integral as it is difficult to compute analytically.
}

(c) Find the probability that $(X, Y, Z)$ lies in a smaller sphere of radius 0.5.

\hide{To solve this problem, we note that because the density is uniform, the question of finding
the probability of locating a point in a given region is simply the volume of that region divided by the
total volume, which is $\frac{4\pi}{3}$. Thus, we have:
$$P((X, Y, Z) \in B_{0.5}) = \frac{\frac{4\pi(0.5)^3}{3}}{\frac{4\pi}{3}} = \frac{1}{8}.$$
}

\section{Covariance and Correlation}

\begin{description}

\item[Covariance] $\cov(X, Y) = E[(X - EX)(Y - EY)] = E(XY) - E(X)E(Y)$. The covariance of independent r.v.s is 0.

\item[Key properties of covariance]

\item[Correlation] $\corr(X, Y) = \frac{\cov(X, Y)}{\SD(X)\SD(Y)}$

\end{description}

\section{Multinomial}

\begin{description}

\item[Story] Each of $n$ objects is placed randomly and independently into $k$ categories. The probability of being placed into category $i$ is $p_i$. Let $\mathbf{p} = (p_1, p_2, \dots, p_k)$. Then the distribution of the number of objects in each category is $\Mult_k(n, \mathbf{p})$. This is a generalized version of the Binomial (the first element of a $\Mult_2(n, \mathbf{p})$ is distributed as a $\Bin(n, p_1)$).

\item[Joint PMF] $$P(X_1 = n_1, X_2 = n_2, \dots, X_k = n_k) = \frac{n!}{n_1!n_2!\cdots n_k!}p_1^{n_1}p_2^{n_2}\cdots p_k^{n_k}$$ You don't really have to remember this; this is the generalization of the Binomial PMF.

\item[Lumping] If you lump more than one of the $k$ categories together, the result is still Multinomial. For example, lumping categories 1 and 2 together results in a Multinomial with new probability $p_1 + p_2$.

\end{description}

\end{document}

