\documentclass{article}

\def\sectionnumber{6}
\def\sectiontitle{Exponential Distribution and Moments}

\usepackage{common}

% Set this false to hide, true to show
\setboolean{showanswers}{true}

\begin{document} 

\header

\section{Exponential Distribution}
\begin{description}

\item[Memoryless property (intuitive explanation and mathematical definition): ]

If $X$ has the memoryless property, then $$P(X \geq s + t| X \geq s) = P(X \geq t).$$ In other words, if you've already waited for $s$ units of time, the probability of having to wait an additional $t$ units of time is the same as the initial (i.e., back when you started waiting) probability that you had to wait $t$ units of time. The Exponential distribution is the only continuous distribution with this property. However, the Geometric distribution also has this property (try proving this as an exercise).

\item[PDF:] The PDF of an $\Expo(\lambda)$ r.v.~is $f(x) = \lambda e^{-\lambda x}$.

\item[CDF:] The CDF of an $\Expo(\lambda)$ r.v.~is $F(x) = 1 - e^{-\lambda x}$.

\end{description}

1. Let $X_1 \sim \Expo(\lambda_1)$ and $X_2 \sim \Expo(\lambda_2)$ be independent. 

(a) What is the PDF of $X = \max(X_1,X_2)$? Does $X$ have an exponential distribution? 

\hide{Note that $X \leq x$ implies ($X_1 \leq x$ and $X_2 \leq x$) and vice versa. Thus, $$P(X \leq x) = P(X_1 \leq x, X_2 \leq x) = P(X_1 \leq x)P(X_2 \leq x) = (1 - e^{-\lambda_1x})(1 - e^{-\lambda_2x}).$$ Taking the derivative of this with respect to $x$ gives $$\lambda_1 e^{-\lambda_1x} + \lambda_2 e^{-\lambda_2x} - (\lambda_1 + \lambda_2)e^{-(\lambda_1 + \lambda_2)x}.$$ Since this doesn't have the form of the Exponential PDF, $X$ isn't Expo.
  
}

(b) Now suppose $\lambda_1 = \lambda_2 = \lambda$. What is $E(X)$? Even better, what is $E(X)$ if $\lambda_1 \neq \lambda_2$? Hint: you don't need to do a messy integral.

\hide{Note that $$E(\min(X_1, X_2)) + E(\max(X_1, X_2)) = E(X_1) + E(X_2) = \frac{1}{\lambda_1} + \frac{1}{\lambda_2}.$$ We already know that $\min(X_1, X_2) \sim \Expo(\lambda_1 + \lambda_2)$, so $$E(\min(X_1, X_2)) = \frac{1}{\lambda_1 + \lambda_2}.$$ Solving for $E(\max(X_1, X_2))$ gives $$E(\max(X_1, X_2)) = \frac{1}{\lambda_1} + \frac{1}{\lambda_2} - \frac{1}{\lambda_1 + \lambda_2}.$$ 

If $\lambda_1 = \lambda_2 = \lambda$, we have

$$\frac{1}{\lambda} + \frac{1}{\lambda} - \frac{1}{2\lambda}=\frac{3}{2\lambda}$$

% If $\lambda_1 = \lambda_2 = \lambda$, then $X$ is just the minimum of two Expos plus the time needed for the remaining Expo (without loss of generality, assume this is $X_2$). So let $T_1 \sim \min(X_1, X_2) \sim \Expo(2\lambda)$, and let $T_2 \sim X_2$ by the memoryless property (since if $X_2$ hasn't finished by $T_1$, then the memoryless property says that it's as if $X_2$ had never started). So $$E(X) = E(T_1) + E(T_2) = \frac{1}{2\lambda} + \frac{1}{\lambda} = \frac{3}{2\lambda},$$ which is consistent with the above result.
    
}


2. Science Center B is under construction, so the only way to get to Stat 110 lecture is through a revolving door that gets stuck every time someone tries to go through it. Suppose the door only fits one person at a time, and the time it takes the person to get through the door (independent for different people) is distributed exponentially with rate $\frac{1}{2}$ inverse seconds. Crystal enters the door on Tuesday at 2:36:56 pm, and Alan arrives at 2:36:58 pm. Nobody arrives between them. What is the probability that Alan gets in the door on Harvard time (i.e. before 2:37:00 pm)?


\hide{We condition on the event $C$ that Crystal is still in the door when Alan arrives (which is equivalent to the event $X > 2$). Let $X \sim \Expo(\frac{1}{2})$ be the time for Crystal to go through the door. Then $P(C) = P(X>2) = 1-P(X \leq 2) = e^{-1}$ by the Expo CDF. Let $Y$ be the time for Alan to get in the door. Then 
    \begin{align*}
        P(Y \leq 2) = P(Y \leq 2|C)P(C) + P(Y \leq 2|C^C)P(C^C)
    \end{align*}
Note that $P(Y \leq 2|C^C) = 1$ since given $C^C$, $Y=0$. Conversely, we have $P(Y \leq 2|C) = P(X \leq 4|C) = P(X \leq 2) = 1-e^{-1}$ by memorylessness. So the answer is $(1-e^{-1})e^{-1} + (1-e^{-1}) = \boxed{1-e^{-2}}$. 
} 

\section{Moments and Moment Generating Functions}

\begin{description}
\item[Moments] The $n$th moment of a r.v.~$X$ is $E(X^n)$. The first 4 moments are mean, variance, skewness (measures symmetry), and kurtosis (measures fatness of tails).

\item[Moment generating function] The \textit{moment generating function} (MGF) of an r.v.~$X$ is $M(t) = E(e^{tX})$, as a function of $t$, if this is finite on some open interval $(-a, a)$ containing 0. Otherwise we say the MGF of $X$ does not exist. $t$ itself has no meaning; to find the $n$th moment, we find $M^{(n)}(0) = E(X^n)$, the $n$th derivative at 0. The MGF uniquely determines the distribution. 

\item[Taylor series of MGF] The MGF can also be expressed as $$M(t) = \sum_{n = 0}^\infty M^{(n)}(0)\frac{t^n}{n!} = \sum_{n = 0}^\infty E(X^n)\frac{t^n}{n!}$$ If you can express an MGF in terms of a Taylor series, you automatically know all the moments by pattern-matching.

\item[Sum of independent r.v.s] The MGF of $X + Y$ with $X$, $Y$ independent is $M_X(t)M_Y(t)$.
\end{description}

3. Let $X \sim N(\mu,\sigma^2)$. What is the MGF of $X$ given that the MGF of a standard Normal is given by $M(t) = e^{\frac{t^2}{2}}$ for all $t$? 

\hide{Let $M_X(t)$ be the MGF of $X$. Then by definition, $M_X(t) = E(e^{Xt})$. But if $Z \sim N(0,1)$, $X$ has the same distribution as $\mu + \sigma Z$, so we have $$
E(e^{Xt}) = E(e^{(\mu+\sigma Z)t}) = E(e^{\mu t}e^{\sigma Zt}) = e^{\mu t}M(\sigma t) = e^{\mu t + \frac{1}{2}\sigma^2t^2}
$$}

4. (a) What is the MGF of $U \sim \Unif(a,b)$? For what values of $t$ does it exist?  

\hide{Let $M(t)$ be the desired MGF. Then by definition, $M(t) = E(e^{tU}) = \int_a^b e^{tu} \frac{1}{b-a} du$ by LOTUS. This evaluates to $\boxed{\frac{e^{bt}-e^{at}}{t(b-a)}}$ when $t \neq 0$. When $t = 0$, the integral evaluates to simply $1$. Since the expression we obtain is finite for all $t$, the MGF exists everywhere.}

(b) Now suppose $U_1$ and $U_2$ are independently $\Unif(a,b)$. Prove that the distribution of $U_1+U_2$ is not uniform using your answer in part (a).

\hide{The MGF of $U_1+U_2$ is simply $M^2(t)$, which clearly doesn't take the same form as $M(t)$.}

(c) Now suppose $a=0$ and $b=1$. Find the moments $E(U^n)$ for all positive integers $n$. 

\hide{The MGF simplifies to $M(t) = \frac{e^t-1}{t}$ which has series expansion $\sum_{n=0}^{\infty} \frac{t^n}{(n+1)!}$. The $n$-th moment $E(U^n)$ is the coefficient in front of $\frac{t^n}{n!}$, which here is just $\boxed{\frac{1}{n+1}}$. }

\end{document}

