\documentclass{article}

\def\sectionnumber{3}
\def\sectiontitle{More Conditioning and Random Variables}

% Set this =0 to hide, =1 to show
\def\showanswers{1}

\usepackage{common}

\begin{document} 

\header

% \section{Monty Hall and More Conditional Probability}

% (BH 2.41) You are the contestant on the Monty Hall show. Monty is trying out a new version of his game, with rules as follows. You get to choose one of three doors. One door has a car behind it, another has a computer, and the other door has a goat (with all permutations equally likely). Monty, who knows which prize is behind each door, will open a door (but not the one you chose) and then let you choose whether to switch from your current choice to the other unopened door.
% Assume that you prefer the car to the computer, the computer to the goat, and (by transitivity) the car to the goat.


% \begin{enumerate}
%     \item Suppose for this part only that Monty always opens the door that reveals your less preferred prize out of the two alternatives, e.g., if he is faced with the choice between revealing the goat or the computer, he will reveal the goat. Monty opens a door, revealing a goat (this is again for this part only). Given this information, should you switch? If you do switch, what is your probability of success in getting the car?
    
%     \hide{Let $C$ be the event that the car is behind the door you originally chosen, and let $M_{goat}$ be the event that Monty reveals a goat when he opens a door. By Bayes' rule,
%     $$P(C|M_{goat}) = \frac{P(M_{goat}|C)P(C)}{P(M_{goat})} = \frac{1\cdot 1/3}{2/3} = 1/2,$$
%     where the denominator comes from the fact that the two doors other than your initial choice are equally likely to have {car, computer}, {computer, goat}, or {car, goat}, and only in the first of these cases will Monty not reveal a goat. So you should be indifferent between switching and not switching; either way, your conditional probability of getting the car is 1/2. (Note though that the unconditional probability that switching would get you the car, before Monty revealed the goat, is 2/3 since you will succeed by switching if and only if your initial door does not have the car.)}
    
%     \item Now suppose that Monty reveals your less preferred prize with probability $p$, and your more preferred prize with probability $q = 1 - p$. Monty opens a door, revealing a computer. Given this information, should you switch (your answer can depend on $p$)? If you do switch, what is your probability of success in getting the car (in terms of $p$)?
    
%     \hide{Let $C, R, G$ be the events that the car is behind the door you originally chosen is a car, computer, goat, respectively, and let $M_{comp}$ be the event that Monty reveals a computer when he opens a door. By Bayes' rule and LOTP,
%     $$P(C|M_{comp}) = \frac{P(M_{comp}|C)P(C)}{P(M_{comp}|C)P(C) + P(M_{comp}|R)P(R) + P(M_{comp}|G)P(G)}.$$
%     We have $P(M_{comp}|C) = q$, $P(M_{comp}|R) = 0$, $P(M_{comp}|G) = p$, so $P(C|M_{comp}) = \frac{q/3}{q/3 + 0 + p/3} = q$.
%     }
% \end{enumerate}

\section{Gambler's Ruin}

\begin{enumerate}[(a)]
    \item $A$ and $B$ start out with $i$ and $N-i$ dollars, respectively. Each round they bet 1 dollar with each other, and $A$ wins with probability $p$. Then the chance that $B$ goes bankrupt first is:
    
    \hide{From the class notes, the answer is $\frac{i}{N}$ if $p = \frac{1}{2}$, and $\frac{1-\left(\frac{q}{p}\right)^i}{1-\left(\frac{q}{p}\right)^N}$ otherwise. Note: put this on your cheat sheet.}

    \item Jimmy flips a fair coin, winning a dollar if the coin lands heads, losing a dollar otherwise. He starts with 10 dollars, and decides to implement a stopping rule whereby he stops playing if he either loses all his money or amasses a wealth of $N$ dollars. Show that no matter what value of $N$ he chooses, Jimmy will end up with the same wealth on average.
    
    \hide{Fixing N, the probability that Jimmy stops due to winning is $\frac{10}{N}$ by the Gambler's Ruin problem. In this case, he has $N$ dollars; otherwise he has 0 dollars. Thus his expected wealth at the end of the game is $\frac{10}{N} \cdot N = 10$ which is independent of $N$ as long as $N \geq 10$.}
    
    \item Now suppose the coin is biased and lands heads with only probability 0.4. How would you find the optimal value of $N$?
    
    \hide{Using the Gambler's ruin result, Jimmy wins with probability $\frac{1-(\frac{3}{2})^{10}}{1-(\frac{3}{2})^N}$. The expected earnings are $N$ times this. This turns out to be a decreasing function of $N$ (can take the derivative to check) so Jimmy shouldn't play at all, i.e. he should set $N=10$.}
    
    \item (BH 2.46) As in the Gambler's Ruin problem, two gamblers, $A$ and $B$, make a series of bets until one of the gamblers goes bankrupt. Let $A$ start out with $i$ dollars and $B$ start with $N-i$ dollars, and let $p$ be the probability of $A$ winning a bet with $0 < p < \frac{1}{2}$. Each bet is for $\frac{1}{k}$ dollars, with $k$ a positive integer, e.g. $k=1$ is the original gambler's ruin problem and $k=20$ means they're betting nickels. Find the probability that $A$ wins the game, and determine what happens to this as $k \rightarrow \infty$.
    
    \hide{
    For the original gambler's ruin problem, if $p_i$ represents the probability $A$ wins starting out with $i$ dollars, then $p_i = \frac{1-\left(\frac{1-p}{p}\right)^i}{1-\left(\frac{1-p}{p}\right)^N}$. But the only difference between that problem and the one here is that the bets are reduced to $\frac{1}{k}$; this is equivalent to the original gambler's ruin problem where $A$ and $B$ start with $ki$ and $k(N-i) = kN-ki$ dollars, respectively (we can see this by just redenominating the currency so that each new dollar is worth $\frac{1}{k}$ old dollars). Thus the desired probability is $\boxed{\frac{1-\left(\frac{1-p}{p}\right)^{ki}}{1-\left(\frac{1-p}{p}\right)^{kN}}}$. Since $0<p<\frac{1}{2}$, $\frac{1-p}{p} > 1$ so as $k$ approaches infinity, $(\frac{1-p}{p})^{ki}$ and $(\frac{1-p}{p})^{kN}$ both grow without bound. But then 

    \begin{align*}
    \lim_{k \rightarrow \infty} \frac{1-\left(\frac{1-p}{p}\right)^{ki}}{1-\left(\frac{1-p}{p}\right)^{kN}} & = \lim_{k \rightarrow \infty} \frac{-\left(\frac{1-p}{p}\right)^{ki}}{-\left(\frac{1-p}{p}\right)^{kN}} \\
    & = \lim_{k \rightarrow \infty} \frac{1}{\left(\frac{1-p}{p}\right)^{k(N-i)}}\\
    & = 0 \text{ since $N>i$.}
    \end{align*}
    }
    
    
\end{enumerate}

\section{PMFs}
Random variable: 

\hide{Given an experiment with sample space $S$, a random variable (r.v.) is a function from the sample space $S$ to the real numbers $\mathbb{R}$. It is common, but not required, to denote random variables by capital letters. Thus, a random variable $X$ assigns a numerical value $X(s)$ to each possible outcome $s$ of the experiment. The randomness comes from the fact that we have a random experiment (with probabilities described by the probability function $P$); the mapping itself is deterministic.}

Probability Mass Function: 

\hide{The probability mass function (PMF) of a discrete r.v.~$X$ is the function $p_X$ given by $p_X(x) = P(X = x)$. Note that this is positive if $x$ is in the support of $X$, and 0 otherwise.}

Bernoulli r.v.~story and PMF: 

\hide{Imagine a coin that lands heads with probability $p$. Record a 1 if the coin is heads and 0 otherwise. This is the story of a $\Bern(p)$ r.v.~You can also imagine any binary event that happens with probability $p$. The PMF is $P(X = 1) = p$ and $P(X = 0) = 1 - p$.}

Binomial r.v.~story and PMF: 

\hide{Imagine flipping a coin that lands heads with probability $p$ $n$ times. Then the number of heads is $\Bin(n, p)$. The PMF is $$P(X = k) = \binom{n}{k} p^k ( 1 - p )^{n - k}$$ for $k = 0,1,\cdots,n$, and $P(X = k) = 0$ otherwise.}

Annie rolls three dice. Every time she rolls at least a 4, she flips a fair coin. What is the PMF of the number of heads she flips?

\hide{Define $X$ to be the number of heads Annie flips. Clearly the support of $X$ is $\{0,1,2,3\}$. Now let $Y$ be the number of times Annie rolls at least a 4, or equivalently the number of times Annie flips a coin; then $Y \sim \Bin(3,\frac{1}{2}) $. We can now compute the PMF using LOTP. First 
\begin{align*}
    P(X=3) & = P(X=3,Y=3) \\
    & = P(X=3|Y=3)P(Y=3) \\
    & = \frac{1}{8} \cdot \frac{1}{8} \\
    & = \frac{1}{64} \\
    P(X=2) & = P(X=2|Y=3)P(Y=3) + P(X=2|Y=2)P(Y=2) \\
    & = \frac{3}{8} \cdot \frac{1}{8} + \frac{1}{4} \cdot \frac{3}{8} =\\
    & = \frac{9}{64} \\
    P(X=1) & = P(X=1|Y=3)P(Y=3) + P(X=1|Y=2)P(Y=2) + P(X=1|Y=1)P(Y=1) \\
    & = \frac{3}{8} \cdot \frac{1}{8} + \frac{1}{2} \cdot \frac{3}{8} + \frac{1}{2} \cdot \frac{3}{8} \\
    & = \frac{27}{64}
\end{align*}
Then $P(X=0) = 1-P(X=1)-P(X=2)-P(X=3) = \frac{27}{64}$}

% \section{A Simple Random Walk}

% A very drunk man goes for a walk. He sets off on a random walk along a line: at each step, he flips a fair coin (somehow) and either takes one step to the right of his current position or takes one step to the left of his current position, depending on whether the coin lands heads or tails.

% \begin{enumerate}[(a)]
%     \item What is the probability that after $n$ steps, he ends up exactly back in his original location?
    
%     \hide{What needs to occur in order for the man to return to his original location? At some point(s), he needs to take $n/2$ steps to the left and $n/2$ steps to the right (note that this event has probability 0 if $n$ is odd). Thus, we see that the number of ways that the man can end up back at his original location is simply $\binom{n}{n/2}$. But the total number of ways the man can walk is simply $2^n$, since he has a choice of right or left at every point. Thus, we have $$P(\textrm{original location}) = \frac{\binom{n}{n/2}}{2^n}.$$}
    
%     \item It just so happens that at one end of the street, 2 steps to the man's left, there is a bar and at the other end of the street, 3 steps to the man's right, there is an AA clinic. Calculate the probability that the man will reach the bar before the AA clinic.
    
%     \hide{We can denote the left end of the street (i.e. the bar) as 0, the right end (i.e. the AA clinic) as $N = 5$, and the man's original position as $i = 2$. Thus, the probability of the man reaching the AA clinic is, by the Gambler's Ruin formula, $\frac{i}{N} = \frac{2}{5}$.}
% \end{enumerate}

\section{Properties of the Binomial}

\begin{enumerate}[(a)]
    \item Suppose $X \sim \Bin(n_1, p)$ and independently $Y \sim \Bin(n_2, p)$. What is the distribution of $X + Y$? Explain.
    
    \hide{Let's think intuitively: $\Bin(n_1, p)$ means that we are considering $n_1$ independent Bernoulli trials with probability $p$. Similarly, $\Bin(n_2, p)$ is a result of $n_2$ independent Bernoulli trials, also independent from the original $n_1$ trials, with the same probability $p$. Thus, $X + Y$ is the same thing as $n_1 + n_2$ independent Bernoulli trials with probability $p$, and $X + Y \sim \Bin(n_1 + n_2, p)$.}
    
    \item What is $P(X \geq 2)$?
    
    \hide{Using complements gives $P(X \geq 2) = 1 - P(X = 0) - P(X = 1) = 1 - \binom{n_1}{0}(1 - p)^{n_1} - \binom{n_1}{1}p^1(1 - p)^{n_1 - 1} = 1 - (1 - p)^{n_1} - n_1p(1 - p)^{n_1 - 1}$.}
    
    \item What is $P(X + Y = 10)$? Assume that $n_1 + n_2 \geq 10$.
    
    \hide{Using the answer to the first part gives $P(X + Y = 10) = \binom{n_1 + n_2}{10}p^{10}(1 - p)^{n_1 + n_2 - 10}$.}
    
    \item What is one reason why $X - Y$ cannot be Binomial?
    
    \hide{The clearest violation of the assumptions that we make in the Binomial model is that $X - Y$ can be negative. Recall that Binomial distributions can only take on non-negative integer values. (This is why the \textit{support} is so important!) Thus, $X - Y$ cannot be Binomial.}
    
    \item Can you construct two random variables $X$ and $Y$ both distributed $\Bin(3, \frac{1}{2})$ such that $P(X = Y ) = 0$?
    
    \hide{The trick is to consider completely dependent/correlated $X$, $Y$. Consider $X \sim \Bin(3, 1/2)$, and let $Y = 3 - X$. Clearly it is never the case that $X = Y$.}
    
    \item *Dylan and Kevin are really bored, so they decide to play a game where Dylan flips 2016 fair coins and Kevin flips 2017 fair coins. What is the probability that Kevin flips more heads than Dylan? 
    
    \hide{Let $K$ be the number of heads Kevin flips and $D$ be the number of heads Dylan flips. Then $K \sim \Bin(2017,\frac{1}{2})$ and $D \sim \Bin(2016,\frac{1}{2})$. We want $P(K>D)$, which we can rearrange mathematically as
    \begin{align*}
        P(K>D) & = P(K \geq D+1) \\
        & = P(2017-K \geq D+1) \text{ by symmetry} \\
        & = P(D+K \leq 2016)
    \end{align*}
    But $D+K \sim \Bin(4033,\frac{1}{2})$, and we have $P(D+K=a) = P(D+K=4033-a)$ for all $a \in \{0,1,...,4033\}$, so the answer is just $\boxed{\frac{1}{2}}$. \\
    An alternative solution is to first consider Kevin's first 2016 flips compared to all of Dylan's flips. Let $H$ be the event Kevin has more heads after the first 2016 flips than Dylan, $h=P(H)$. By symmetry, $h$ is also the probability Kevin has less heads than Dylan after the first 2016 flips (call the event $L$). Finally let $S$ be the event they have the same number of heads after 2016 flips each. Then by LOTP, the probability Kevin has more flips overall (event $K$) is given by  
    \begin{align*}
        P(K) & = P(K|H)P(H) + P(K|L)P(L) + P(K|S)P(S) \\
        & = 1 \cdot h + 0 \cdot h + \frac{1}{2} \cdot (1-2h) \\
        & = \frac{1}{2}
    \end{align*}}
    
\end{enumerate}

\section{More distributions and CDFs}
Geometric distribution PMF and story: 

\hide{Flip a coin that lands heads with probability $p$. Then the number of tails you'll get before you flip heads for the first time is distributed as $\Geom(p)$. The PMF is given by $P(X=x) = p(1 - p)^x$ for $x \in \{0,1,...\}$ (why?).}

First success distribution PMF and story: 

\hide{Flip a coin that lands heads with probability $p$. Then the number of the flip on which you'll get your first heads is distributed as $\FS(p)$. The PMF is given by $P(X=x) = p(1-p)^{x-1}$ for $x \in \{1,2,...\}$ (why?).}

Hypergeometric distribution PMF and story:

\hide{Let's say there are $w$ white balls and $b$ black balls in a jar. If you randomly pick $n$ of them without replacement, then the number of white balls $X$ you get $\sim \HGeom(w,b,n)$. We have $P(X=x) = \frac{\binom{w}{x} \cdot \binom{b}{n-x}}{\binom{w+b}{n}}$ for $x \in \{0,1,...,n\}$ (why?).}

Definition of CDF: 

\hide{The Cumulative Density Function (CDF) of a r.v.~X is simply defined by $P(X \leq x)$. It's often denoted $F(x)$.} 

\begin{enumerate}[(a)]
\item Let $X \sim \FS(p)$. What is the CDF of $X$ evaluated at any integer $k$? \\
\hide{For nonpositive $k$, the CDF is just 0. For positive $k$, we compute 
\begin{align*}
    P(X \leq k) & = \sum_{i=1}^k P(X=i) \\
    & = \sum_{i=1}^k p(1-p)^{i-1} \\
    & = p \sum_{i=1}^k (1-p)^{i-1} \\
    & = \boxed{(1-(1-p)^k)}
\end{align*}}

\item What is the CDF of $1-X$ evaluated at any integer $k$? \\
\hide{We note
\[
  P(1-X \leq k) = P(X \geq 1-k) = 1-P(X < 1-k) = 1-P(X \leq -k)
\]
If $k$ is nonnegative, then the CDF is just 1. If $k$ is negative by part (a) we can evaluate
\[
1-P(X \leq -k) = \boxed{(1-p)^k}
\]
}
\end{enumerate}


\end{document}

